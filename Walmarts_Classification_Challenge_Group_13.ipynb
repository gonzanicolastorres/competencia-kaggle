{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYoOiEYXoSAJ"
   },
   "source": [
    "# Diplodatos Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeGzAToCoSAK"
   },
   "source": [
    "We present this peace of code to create the baseline for the competition, and as an example of how to deal with these kind of problems. The main goals are that you:\n",
    "\n",
    "1. Learn\n",
    "1. Try different models and see which one fits the best the given data\n",
    "1. Get a higher score than the given one in the current baseline example\n",
    "1. Try to get the highest score in the class :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmzh-QKuoSAL"
   },
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybqdvn85oSAQ"
   },
   "source": [
    "Read the *original* dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YREVk5BLoSAR"
   },
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('https://raw.githubusercontent.com/DiploDatos/AprendizajeSupervisado/master/practico/data/train.csv')\n",
    "\n",
    "url_train = 'https://raw.githubusercontent.com/DiploDatos/AprendizajeSupervisado/master/practico/data/train.csv'\n",
    "url_test = 'https://raw.githubusercontent.com/DiploDatos/AprendizajeSupervisado/master/practico/data/test.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(original_df.Weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(original_df.DepartmentDescription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `nan`s in the column, let us find them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df[original_df.DepartmentDescription.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the description is NaN, then the Upc and FinelineNumber are both NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(original_df.DepartmentDescription.isna().sum(),\n",
    " (original_df.DepartmentDescription.isna() & original_df.Upc.isna() & original_df.FinelineNumber.isna()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df[original_df.Upc.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it may be the case that Upc is NaN but not the description..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(original_df.Upc.isna().sum(),\n",
    " original_df.FinelineNumber.isna().sum(),\n",
    " (original_df.FinelineNumber.isna() & original_df.Upc.isna()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upc and FinelineNumber are both NaN at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upc and FinelineNumber are both NaN at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = set(original_df.TripType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the TripType values, we have to group the visits by VisitNumber\n",
    "original_df.groupby(\"VisitNumber\").TripType.mean().value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbalanced!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to load the datasets.\n",
    "Some important decisions made here:\n",
    "We will use one hot encoding for Weekday and DepartmentDescription. All transformations are applied to the training and testing datasets...\n",
    "We are handling NaN's as another category. This may not be the best approach.\n",
    "We may have multiple records for one single visit and the goal is to classify all those records the exact same way. Therefore, we will prepare the data in a way that all the information for a visit is in the same record.\n",
    "Based on the last bullet, we will count the DepartmentDescription for all items acquired in the same visit.\n",
    "We drop the Upc and FinelineNumber to simplify the process. You may use it as they have a lot of information (may be using one-hot encoding for them as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operations will be performed in a function. Anyway, we present them here so that we can see them in action:\n",
    "First, we drop the columns. We include TripType as we are going to treat it differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.drop([\"Upc\", \"FinelineNumber\", \"TripType\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we group by the VisitNumber and Weekday (they should be the same), and add all values for ScanCount, and the one-hot encoding of DepartmentDescriptioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_original(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df = df.drop([\"Upc\", \"FinelineNumber\"], axis=1)\n",
    "\n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df = pd.get_dummies(df, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"], dummy_na=True)\n",
    "\n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data_original(url_train, url_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of comparison\n",
    "models_scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training dataset into train and \"validation\" \n",
    "# (we won't be using validation set in this example, because of the cross-validation;\n",
    "# but it could be useful for you depending on your approach)\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results dataframe is used to store the computed results\n",
    "results = pd.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a DesicionTree to classify and GridSearch to determine the parameters\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_param = {'criterion':('gini', 'entropy'), 'min_samples_leaf':(1, 2, 5),\n",
    "              'min_samples_split':(2, 3, 5, 10, 50, 100)}\n",
    "tree = DT(random_state=42)\n",
    "tree_clf = GridSearchCV(tree, tree_param, cv=3, scoring='accuracy') #scoring='balanced_accuracy')\n",
    "tree_clf.fit(X_train, y_train)\n",
    "best_tree_clf = tree_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Decision Tree accuracy: ', tree_clf.best_score_)\n",
    "print(best_tree_clf)\n",
    "results = results.append({'clf': best_tree_clf, 'best_acc': tree_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_acc'].idxmax()]['clf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And finally**, we predict the unknown label for the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores = results\n",
    "models_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = results.clf.iloc[0].predict(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we do is generating a file that should be *submitted* on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../data/submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQCLhhEq2Tko"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, Group 13, propose the following classification solutions for Walrmat's Classification Challenge. Many improvement can be included but resourcer are scarse and hence not many alternatives could be tested nor parameters of the selected models tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_department_counts(data):\n",
    "    alist = []\n",
    "    for array in np.asarray(data.iloc[:,4:70]):\n",
    "        count = 0\n",
    "        for item in array:\n",
    "            if item > 0:\n",
    "                count += 1\n",
    "        alist.append(count)\n",
    "    dept_counts = pd.DataFrame(alist)\n",
    "    dept_counts = dept_counts.rename(columns={0:\"DepartmentCounts\"})\n",
    "    dept_counts = dept_counts.set_index(data.index)\n",
    "    data.insert(4, 'DepartmentCounts', dept_counts)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    df_test = pd.read_csv(test_data_fname) \n",
    "    df_test['is_train_set'] = 0\n",
    "\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).max().TripType\n",
    "\n",
    " \n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "\n",
    "\n",
    "   # Corrections of misspelling in some department's names \n",
    "    df['DepartmentDescription'] = df_2['DepartmentDescription'].replace({'GIRLS WEAR, 4-6X  AND 7-14': 'LADIES WEAR',\n",
    "               'HEALTH AND BEAUTY AIDS': 'HEALTH & BEAUTY', 'LADIESWEAR': 'LADIES WEAR',\n",
    "               'MENSWEAR' : 'MENS WEAR', 'OPTICAL - FRAMES' : 'OPTICAL',\n",
    "               'OPTICAL - LENSES': 'OPTICAL', 'PHARMACY OTC': 'PHARMACY',\n",
    "               'PHARMACY OTC': 'PHARMACY'})\n",
    "\n",
    "    dummies = pd.get_dummies(df.DepartmentDescription)\n",
    "    df[dummies.columns] = dummies \n",
    "    df['Weekday'] = df['Weekday'].map({\"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3, \"Thursday\": 4, \"Friday\": 5, \n",
    "                                           \"Saturday\": 6, \"Sunday\": 7})\n",
    "    data_dummies = df.iloc[:,7:]\n",
    "    data_dummies = data_dummies.apply(lambda x: x*df[\"ScanCount\"])\n",
    "    data_dummies = data_dummies.replace(-0,0)\n",
    "    \n",
    "    \n",
    "    # FinelineNumber \n",
    "    # 0.7 da 0.74514\n",
    "    # 0.6 da 0.74573\n",
    "    # The percentile selected filters certain amount of data. It should be tuned in order to keep just the most\n",
    "    # Important values. Applying the same approach to other features copuld be interesting. \n",
    "    fineline_ranks = df.FinelineNumber.value_counts()\n",
    "    fineline_ranks = fineline_ranks[fineline_ranks > fineline_ranks.quantile(0.6)]\n",
    "    columns1 = list(fineline_ranks.index)\n",
    "    dummies = pd.get_dummies(df.FinelineNumber)\n",
    "    dummies = dummies[columns1]\n",
    "\n",
    "\n",
    "    df.loc[df.ScanCount < 0, 'ItemsReturned'] = 1\n",
    "    df.loc[df.ItemsReturned != 1, 'ItemsReturned'] = 0\n",
    "    \n",
    "    df = df[[ \"VisitNumber\", \"Weekday\", \"Upc\", \"ScanCount\", \"ItemsReturned\", \"is_train_set\"]]\n",
    "    df = df.rename(columns={\"ScanCount\":\"NumItems\"})\n",
    "    df = pd.concat([df, data_dummies], axis=1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "    # now we add the groupby values\n",
    "    df = df.groupby([\"VisitNumber\", \"Weekday\"], as_index=False).sum()\n",
    "    \n",
    "    df['working_day'] =  df['Weekday'].map({1: 'working_day', 2: 'working_day', 3: 'working_day',\n",
    "                                        4: 'working_day', 5: 'working_day', \n",
    "                                           6: 'weekend', 6: 'weekend'})\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"working_day\"])\n",
    "\n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df = pd.get_dummies(df, columns=[\"Weekday\"])\n",
    "    \n",
    "    \n",
    "    # get train and test back\n",
    "    df_train = df[df.is_train_set != 0]\n",
    "    df_test = df[df.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "\n",
    "    return X, y, XX, yy\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, XX, yy = transform_data_fineline(\"https://raw.githubusercontent.com/DiploDatos/AprendizajeSupervisado/master/practico/data/train.csv\", \"https://raw.githubusercontent.com/DiploDatos/AprendizajeSupervisado/master/practico/data/test.csv\")\n",
    "\n",
    "# Encode labels?\n",
    "add_department_counts = True\n",
    "\n",
    "if (add_department_counts):\n",
    "    X = add_department_counts(X)\n",
    "    XX = add_department_counts(XX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>Upc</th>\n",
       "      <th>NumItems</th>\n",
       "      <th>Return</th>\n",
       "      <th>DepartmentCounts</th>\n",
       "      <th>1-HR PHOTO</th>\n",
       "      <th>ACCESSORIES</th>\n",
       "      <th>AUTOMOTIVE</th>\n",
       "      <th>BAKERY</th>\n",
       "      <th>BATH AND SHOWER</th>\n",
       "      <th>...</th>\n",
       "      <th>9974.0</th>\n",
       "      <th>working_day_weekend</th>\n",
       "      <th>working_day_working_day</th>\n",
       "      <th>Weekday_1</th>\n",
       "      <th>Weekday_2</th>\n",
       "      <th>Weekday_3</th>\n",
       "      <th>Weekday_4</th>\n",
       "      <th>Weekday_5</th>\n",
       "      <th>Weekday_6</th>\n",
       "      <th>Weekday_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6.811315e+10</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1.070088e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1.700927e+10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2.273895e+10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>1.832173e+11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   VisitNumber           Upc  NumItems  Return  DepartmentCounts  1-HR PHOTO  \\\n",
       "0            5  6.811315e+10        -1     1.0                 0           0   \n",
       "3            9  1.070088e+09         3     0.0                 3           0   \n",
       "4           10  1.700927e+10         3     0.0                 2           0   \n",
       "5           11  2.273895e+10         4     0.0                 3           0   \n",
       "6           12  1.832173e+11         7     0.0                 4           0   \n",
       "\n",
       "   ACCESSORIES  AUTOMOTIVE  BAKERY  BATH AND SHOWER  ...  9974.0  \\\n",
       "0            0           0       0                0  ...       0   \n",
       "3            0           0       0                0  ...       0   \n",
       "4            0           0       0                0  ...       0   \n",
       "5            0           0       0                0  ...       0   \n",
       "6            0           0       0                0  ...       0   \n",
       "\n",
       "   working_day_weekend  working_day_working_day  Weekday_1  Weekday_2  \\\n",
       "0                    0                        1          0          0   \n",
       "3                    0                        1          0          0   \n",
       "4                    0                        1          0          0   \n",
       "5                    0                        1          0          0   \n",
       "6                    0                        1          0          0   \n",
       "\n",
       "   Weekday_3  Weekday_4  Weekday_5  Weekday_6  Weekday_7  \n",
       "0          0          0          1          0          0  \n",
       "3          0          0          1          0          0  \n",
       "4          0          0          1          0          0  \n",
       "5          0          0          1          0          0  \n",
       "6          0          0          1          0          0  \n",
       "\n",
       "[5 rows x 2146 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels?\n",
    "econd_labels = True\n",
    "\n",
    "if (econd_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y_encoded = le.transform(y)\n",
    "\n",
    "\n",
    "# Create training and test datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts data to a DMatrix on the fly, there's no need to do it ourselves but if corrsvalidation is to be done, it's a necessary step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB 0.7457357402158238\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(seed=1234, objective = 'multi:softprob', num_class = 38)\n",
    "xgb_clf.fit(X_train, y_train, verbose=True)\n",
    "xgb_clf_score = accuracy_score(y_valid, xgb_clf.predict(X_valid))\n",
    "\n",
    "print('XGB', xgb_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 40, 21, ...,  8, 39, 39])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We must transform the encoded labels back to their original values in order to be submitted. \n",
    "yy = le.inverse_transform(xgboost.predict(XX))\n",
    "\n",
    "#yy = xgboost.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission2 = pd.DataFrame(list(zip(XX.VisitNumber, yy)), columns=[\"VisitNumber\", \"TripType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission2.to_csv(\"../data/submission_Alessio_FinelineNumber_8.csv\", header=True, index=False)\n",
    "\n",
    "46920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This were the results we submitted to Kaggle Competition last Friday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to tune the parameters of a XGBoost model. Here we have just tuned the learning rate because if more paramereters are to be included, more iterations are needed and we currently don't have enough resourcer to perform the latter in a reasonable time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from xgboost import XGBRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost hyper-parameter tuning\n",
    "def hyperParameterTuning(X_train, y_train):\n",
    "    param_tuning = {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        #'max_depth': [3, 5, 7, 10],\n",
    "        'objective' : ['multi:softprob'],\n",
    "        #'min_child_weight': [1, 3, 5],\n",
    "        #'subsample': [0.5, 0.7],\n",
    "        #'colsample_bytree': [0.5, 0.7],\n",
    "        #'n_estimators' : [100, 200, 500],\n",
    "         'num_class' : [38]\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "    gsearch = GridSearchCV(estimator = xgb_model,\n",
    "                           param_grid = param_tuning,                        \n",
    "                           scoring = 'accuracy', #MAE\n",
    "                           #scoring = 'reg:squarederror',  #MSE\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1)\n",
    "\n",
    "    gsearch.fit(X_train,y_train)\n",
    "\n",
    "    return gsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 255.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'num_class': 38, 'objective': 'multi:softprob'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run only in the first run of the kernel.\n",
    "hyperParameterTuning(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a cross validation of the hyperparameters for a XGBoost model. Here we are need to transform our data to a DMatrix because the cv function does'nt do it on the fly. Accuracy can't be used here so we are using the error score for multiclassifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dmatrix = xgb.DMatrix(X_train, label = y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_valid, label = y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter dictionary: params\n",
    "param = {'objective': 'multi:softprob', 'num_class':38, \n",
    "     'eval_metric': 'error', \"max_delta_step\": 5, \"learning_rate\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain = train_dmatrix, params = param, \n",
    "                  nfold = 3, num_boost_round = 10, \n",
    "                  metrics = \"merror\", as_pandas = True, seed = 123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-merror-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle_randomforest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
